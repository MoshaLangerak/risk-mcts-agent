{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a802c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee3c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a CSV file into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def load_action_data(file_paths: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load action data from multiple CSV files into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_paths (list[str]): List of paths to the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the combined action data.\n",
    "    \"\"\"\n",
    "    data_frames = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        data_frames.append(df)\n",
    "\n",
    "    return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "def load_player_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load player data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the player data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, converters={'root_scores': ast.literal_eval, 'best_child_scores': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2202dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_win_rate_per_player_type(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the win rates for each player type.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the game data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with player types as keys and their win rates as values.\n",
    "    \"\"\"\n",
    "    player_types = set()\n",
    "\n",
    "    for i in range(4):\n",
    "        player_type_col = f'player_{i}_type'\n",
    "        player_types.add(data[player_type_col].unique()[0])\n",
    "\n",
    "    win_rates = {}\n",
    "\n",
    "    for player_type in player_types:\n",
    "        player_wins = data[data['winner_type'] == player_type]['game_id'].nunique()\n",
    "        total_runs = data['game_id'].nunique()\n",
    "        win_rate = player_wins / total_runs if total_runs > 0 else 0\n",
    "        win_rates[player_type] = win_rate\n",
    "\n",
    "    return win_rates\n",
    "\n",
    "def calculate_win_rate_per_starting_position(data: pd.DataFrame) -> dict:\n",
    "    wins_per_starting_position = data[['winner', 'game_id']].drop_duplicates().groupby('winner').count().reset_index().rename(columns={'game_id': 'count'}).set_index('winner').to_dict()['count']\n",
    "    total_runs = data['game_id'].nunique()\n",
    "    win_rates = {position: wins / total_runs for position, wins in wins_per_starting_position.items()}\n",
    "    return win_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48b962",
   "metadata": {},
   "source": [
    "### Win rate for single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'MCTS_vs_random-10s'\n",
    "data_path = f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{experiment_name}/game_level.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21709e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7415c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates_per_player_type = calculate_win_rate_per_player_type(data)\n",
    "win_rates_per_starting_position = calculate_win_rate_per_starting_position(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_win_rates_per_player_type(win_rates: dict, title='Win Rate by Player Type') -> None:\n",
    "    \"\"\"\n",
    "    Plot the win rate by type from the provided DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the data to plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bars = plt.bar(win_rates.keys(), win_rates.values(), color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Player Type')\n",
    "    plt.ylabel('Win Rate')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add win rate values above bars\n",
    "    for bar, (key, value) in zip(bars, win_rates.items()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_win_rates_per_starting_position(win_rates: dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot the win rate by starting position from the provided DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the data to plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bars = plt.bar(win_rates.keys(), win_rates.values(), color='lightgreen')\n",
    "    plt.title('Win Rate by Starting Position')\n",
    "    plt.xlabel('Starting Position')\n",
    "    plt.ylabel('Win Rate')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add win rate values above bars\n",
    "    for bar, (key, value) in zip(bars, win_rates.items()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_win_rates_per_player_type(win_rates_per_player_type, title=f'Win Rate by Player Type - {experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_win_rates_per_starting_position(win_rates_per_starting_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde400e",
   "metadata": {},
   "source": [
    "### Action distribution for single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'MCTS_vs_random-4s'\n",
    "data_path = f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{experiment_name}/action_level/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_action_data([os.path.join(data_path, f) for f in csv_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83425b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c38903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_action_occurrences(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Count the occurrences of each action in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the action data.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: Series with actions as index and their counts as values.\n",
    "    \"\"\"\n",
    "    return data['action_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ce7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_occurences = count_action_occurrences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01db521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_action_occurences(action_counts: pd.Series, title: str ='Action Occurrences') -> None:\n",
    "    \"\"\"\n",
    "    Plot the occurrences of actions from the provided Series.\n",
    "\n",
    "    Parameters:\n",
    "    action_counts (pd.Series): Series containing action counts.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    bars = plt.bar(action_counts.index, action_counts.values, color='coral')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Action Type')\n",
    "    plt.ylabel('Occurrences')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add occurrence values above bars\n",
    "    for bar, count in zip(bars, action_counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{count}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_occurences(action_occurences, title=f'Action Occurrences - {experiment_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618690a",
   "metadata": {},
   "source": [
    "### Average action distribution per player type for a single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'MCTS_vs_random-5s'\n",
    "\n",
    "action_data_path = f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{experiment_name}/action_level/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_csv_files = [f for f in os.listdir(action_data_path) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_data = load_action_data([os.path.join(action_data_path, f) for f in action_csv_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e60008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_action_occurence_per_turn_per_player_type(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the average action occurrence per turn for each player type.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the action data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with player types as index and action types as columns, \n",
    "                 containing average occurrences per turn.\n",
    "    \"\"\"\n",
    "    # First, count actions per turn per player type per action type\n",
    "    action_counts = data.groupby(['game_id', 'turn', 'player_type', 'action_type']).size().reset_index(name='count')\n",
    "\n",
    "    # Then calculate the average count per turn for each player type and action type\n",
    "    result = action_counts.groupby(['player_type', 'action_type'])['count'].mean().reset_index()\n",
    "\n",
    "    result['average_per_turn'] = result['count']  # Rename for clarity\n",
    "    result = result.pivot(index='player_type', columns='action_type', values='average_per_turn').fillna(0)\n",
    "\n",
    "    result['Total'] = result.sum(axis=1)\n",
    "\n",
    "    # round all values to 1 decimal place\n",
    "    result = result.round(1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = average_action_occurence_per_turn_per_player_type(action_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f737c3d",
   "metadata": {},
   "source": [
    "### Compare win rates across multiple experiments (plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083aba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_names = [\n",
    "    'MCTS_vs_random-1s',\n",
    "    'MCTS_vs_random-2s',\n",
    "    'MCTS_vs_random-3s',\n",
    "    'MCTS_vs_random-4s',\n",
    "    'MCTS_vs_random-5s',\n",
    "    # 'MCTS_vs_random-10s',\n",
    "    # 'MCTS_vs_random-20s',\n",
    "    'MCTS_vs_basic_heuristic-1s',\n",
    "    'MCTS_vs_basic_heuristic-2s',\n",
    "    'MCTS_vs_basic_heuristic-3s',\n",
    "    'MCTS_vs_basic_heuristic-4s',\n",
    "    'MCTS_vs_basic_heuristic-5s',\n",
    "    'MCTS_vs_basic_evaluation-1s',\n",
    "    'MCTS_vs_basic_evaluation-2s',\n",
    "    'MCTS_vs_basic_evaluation-3s',\n",
    "    'MCTS_vs_basic_evaluation-4s',\n",
    "    'MCTS_vs_basic_evaluation-5s'\n",
    "]\n",
    "\n",
    "# Define the experiments with their names, player types, and think time parameters\n",
    "experiments = [\n",
    "    ('MCTS_vs_random-1s', 'Random', 1),\n",
    "    ('MCTS_vs_random-2s', 'Random', 2),\n",
    "    ('MCTS_vs_random-3s', 'Random', 3),\n",
    "    ('MCTS_vs_random-4s', 'Random', 4),\n",
    "    ('MCTS_vs_random-5s', 'Random', 5),\n",
    "    ('MCTS_vs_random-10s', 'Random', 10),\n",
    "    ('MCTS_vs_random-20s', 'Random', 20),\n",
    "    ('MCTS_vs_basic_heuristic-1s', 'Basic Heuristic', 1),\n",
    "    ('MCTS_vs_basic_heuristic-2s', 'Basic Heuristic', 2),\n",
    "    ('MCTS_vs_basic_heuristic-3s', 'Basic Heuristic', 3),\n",
    "    ('MCTS_vs_basic_heuristic-4s', 'Basic Heuristic', 4),\n",
    "    ('MCTS_vs_basic_heuristic-5s', 'Basic Heuristic', 5),\n",
    "    ('MCTS_vs_basic_evaluation-1s', 'Basic Evaluation', 1),\n",
    "    ('MCTS_vs_basic_evaluation-2s', 'Basic Evaluation', 2),\n",
    "    ('MCTS_vs_basic_evaluation-3s', 'Basic Evaluation', 3),\n",
    "    ('MCTS_vs_basic_evaluation-4s', 'Basic Evaluation', 4),\n",
    "    ('MCTS_vs_basic_evaluation-5s', 'Basic Evaluation', 5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates_all_experiments = {}\n",
    "\n",
    "for experiment in experiments_names:\n",
    "    data_path = f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{experiment}/game_level.csv'\n",
    "    data = load_data(data_path)\n",
    "    win_rates_per_player_type = calculate_win_rate_per_player_type(data)\n",
    "    win_rates_all_experiments[experiment] = win_rates_per_player_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379611e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates_all_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_win_rates_experiments(win_rates: dict[str, dict], experiments: list[tuple[str, str, int]], title: str = 'Win Rates Across Experiments') -> None:\n",
    "    \"\"\"\n",
    "    Plot the win rates across different experiments.\n",
    "\n",
    "    Parameters:\n",
    "    win_rates (Dict[str, dict]): Dictionary with experiment names as keys and win rates as values.\n",
    "    \"\"\"\n",
    "    # Process the data to create a DataFrame suitable for plotting\n",
    "    plot_data = []\n",
    "    for exp_name, opponent, think_time in experiments:\n",
    "        # Get the MCTS player's win rate for the experiment, defaulting to 0 if not found\n",
    "        mcts_win_rate = win_rates_all_experiments[exp_name].get('mcts', 0)\n",
    "        plot_data.append({\n",
    "            'opponent_type': opponent,\n",
    "            'think_time': think_time,\n",
    "            'mcts_win_rate': mcts_win_rate\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # --- Create the Plot ---\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "    # Define colors for different opponents for better visual distinction\n",
    "    opponent_colors = {\n",
    "        'Random': 'dodgerblue',\n",
    "        'Basic Heuristic': 'coral',\n",
    "        'Basic Evaluation': 'lightgreen',\n",
    "        'black': 'black'\n",
    "    }\n",
    "\n",
    "    # Plot a separate line for each opponent type\n",
    "    for opponent, group in df.groupby('opponent_type'):\n",
    "        # Sort by think time to ensure the line connects points correctly\n",
    "        group = group.sort_values('think_time')\n",
    "        plt.plot(group['think_time'], group['mcts_win_rate'],\n",
    "                marker='o', linestyle='-', label=opponent,\n",
    "                color=opponent_colors.get(opponent, 'black'),\n",
    "                linewidth=2, markersize=8)\n",
    "\n",
    "    plt.title(title, fontsize=16, pad=20)\n",
    "    plt.xlabel('Think Time (seconds)', fontsize=12)\n",
    "    plt.ylabel('MCTS Win Rate', fontsize=12)\n",
    "    plt.legend(title='Opponent Type', fontsize=10)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xticks(np.arange(1, 21, 1)) # Ensure x-axis ticks are integers from 1 to 20\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_win_rates_experiments(win_rates_all_experiments, experiments, title='MCTS Player Win Rate For Different Opponents and Think Times')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5814dd",
   "metadata": {},
   "source": [
    "### Calculate all win rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_names = [\n",
    "    'MCTS_vs_random-1s',\n",
    "    'MCTS_vs_random-2s',\n",
    "    'MCTS_vs_random-3s',\n",
    "    'MCTS_vs_random-4s',\n",
    "    'MCTS_vs_random-5s',\n",
    "    'MCTS_vs_random-2s-hp',\n",
    "    'MCTS_vs_random-5s-hp',\n",
    "    'MCTS_vs_random-2s-hp-rafa',\n",
    "    'MCTS_vs_random-ib',\n",
    "    'MCTS_vs_random-ib-hp',\n",
    "    'MCTS_vs_random-ib-hp-rafa',\n",
    "    'MCTS_vs_basic_heuristic-1s',\n",
    "    'MCTS_vs_basic_heuristic-2s',\n",
    "    'MCTS_vs_basic_heuristic-3s',\n",
    "    'MCTS_vs_basic_heuristic-4s',\n",
    "    'MCTS_vs_basic_heuristic-5s',\n",
    "    'MCTS_vs_basic_heuristic-2s-hp',\n",
    "    'MCTS_vs_basic_heuristic-5s-hp',\n",
    "    'MCTS_vs_basic_heuristic-2s-hp-rafa',\n",
    "    'MCTS_vs_basic_heuristic-ib',\n",
    "    'MCTS_vs_basic_heuristic-ib-hp',\n",
    "    'MCTS_vs_basic_heuristic-ib-hp-rafa',\n",
    "    'MCTS_vs_basic_evaluation-1s',\n",
    "    'MCTS_vs_basic_evaluation-2s',\n",
    "    'MCTS_vs_basic_evaluation-3s',\n",
    "    'MCTS_vs_basic_evaluation-4s',\n",
    "    'MCTS_vs_basic_evaluation-5s',\n",
    "    'MCTS_vs_basic_evaluation-2s-hp',\n",
    "    'MCTS_vs_basic_evaluation-5s-hp',\n",
    "    'MCTS_vs_basic_evaluation-2s-hp-rafa',\n",
    "    'MCTS_vs_basic_evaluation-ib',\n",
    "    'MCTS_vs_basic_evaluation-ib-hp',\n",
    "    'MCTS_vs_basic_evaluation-ib-hp-rafa',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates_all_experiments = {}\n",
    "\n",
    "for experiment in experiments_names:\n",
    "    data_path = f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{experiment}/game_level.csv'\n",
    "    data = load_data(data_path)\n",
    "    win_rates_per_player_type = calculate_win_rate_per_player_type(data)\n",
    "    win_rates_all_experiments[experiment] = win_rates_per_player_type\n",
    "\n",
    "win_rates_all_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073af5d1",
   "metadata": {},
   "source": [
    "### Analyse number of iterations and depth reached by MCTS player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_names = [\n",
    "    # 'MCTS_vs_random-1s',\n",
    "    # 'MCTS_vs_random-2s',\n",
    "    # 'MCTS_vs_random-3s',\n",
    "    # 'MCTS_vs_random-4s',\n",
    "    # 'MCTS_vs_random-5s',\n",
    "    # 'MCTS_vs_random-2s-hp',\n",
    "    # 'MCTS_vs_random-5s-hp',\n",
    "    # 'MCTS_vs_random-2s-hp-rafa',\n",
    "    # 'MCTS_vs_random-10s',\n",
    "    'MCTS_vs_random-20s',\n",
    "    # 'MCTS_vs_random-ib',\n",
    "    # 'MCTS_vs_random-ib-hp',\n",
    "    # 'MCTS_vs_random-ib-hp-rafa',\n",
    "    # 'MCTS_vs_basic_heuristic-1s',\n",
    "    # 'MCTS_vs_basic_heuristic-2s',\n",
    "    # 'MCTS_vs_basic_heuristic-3s',\n",
    "    # 'MCTS_vs_basic_heuristic-4s',\n",
    "    # 'MCTS_vs_basic_heuristic-5s',\n",
    "    # 'MCTS_vs_basic_heuristic-2s-hp',\n",
    "    # 'MCTS_vs_basic_heuristic-5s-hp',\n",
    "    # 'MCTS_vs_basic_heuristic-2s-hp-rafa',\n",
    "    # 'MCTS_vs_basic_heuristic-ib',\n",
    "    # 'MCTS_vs_basic_heuristic-ib-hp',\n",
    "    # 'MCTS_vs_basic_heuristic-ib-hp-rafa',\n",
    "    # 'MCTS_vs_basic_evaluation-1s',\n",
    "    # 'MCTS_vs_basic_evaluation-2s',\n",
    "    # 'MCTS_vs_basic_evaluation-3s',\n",
    "    # 'MCTS_vs_basic_evaluation-4s',\n",
    "    # 'MCTS_vs_basic_evaluation-5s',\n",
    "    # 'MCTS_vs_basic_evaluation-2s-hp',\n",
    "    # 'MCTS_vs_basic_evaluation-5s-hp',\n",
    "    # 'MCTS_vs_basic_evaluation-2s-hp-rafa',\n",
    "    # 'MCTS_vs_basic_evaluation-ib',\n",
    "    # 'MCTS_vs_basic_evaluation-ib-hp',\n",
    "    # 'MCTS_vs_basic_evaluation-ib-hp-rafa',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_names = [\n",
    "    'MCTS_vs_random-1s',\n",
    "    'MCTS_vs_random-2s',\n",
    "    'MCTS_vs_random-3s',\n",
    "    'MCTS_vs_random-4s',\n",
    "    'MCTS_vs_random-5s',\n",
    "    'MCTS_vs_basic_heuristic-1s',\n",
    "    'MCTS_vs_basic_heuristic-2s',\n",
    "    'MCTS_vs_basic_heuristic-3s',\n",
    "    'MCTS_vs_basic_heuristic-4s',\n",
    "    'MCTS_vs_basic_heuristic-5s',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_root_visit_count(data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average number of root visits from the MCTS player data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing MCTS player data.\n",
    "\n",
    "    Returns:\n",
    "        float: The average number of root visits.\n",
    "    \"\"\"\n",
    "    return data['root_visit_count'].mean()\n",
    "\n",
    "def calculate_average_max_depth_reached(data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average maximum depth reached from the MCTS player data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing MCTS player data.\n",
    "\n",
    "    Returns:\n",
    "        float: The average maximum depth reached.\n",
    "    \"\"\"\n",
    "    return data['max_depth_reached'].mean()\n",
    "\n",
    "def calculate_median_max_depth_reached(data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the median maximum depth reached from the MCTS player data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing MCTS player data.\n",
    "    \n",
    "    Returns:\n",
    "        float: The median maximum depth reached.\n",
    "    \"\"\"\n",
    "    return data['max_depth_reached'].median()\n",
    "\n",
    "def calculate_max_max_depth_reached(data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the maximum depth reached from the MCTS player data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing MCTS player data.\n",
    "    \n",
    "    Returns:\n",
    "        float: The maximum depth reached.\n",
    "    \"\"\"\n",
    "    return data['max_depth_reached'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_visit_counts_per_experiment = {}\n",
    "avg_max_depth_reached_per_experiment = {}\n",
    "med_max_depth_reached_per_experiment = {}\n",
    "max_max_depth_reached_per_experiment = {}\n",
    "\n",
    "for experiment in experiments_names:\n",
    "    data_path = os.listdir(f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{experiment}/player_level/')\n",
    "\n",
    "    root_visit_counts = []\n",
    "    avg_max_depths = []\n",
    "    med_max_depths = []\n",
    "    max_max_depths = []\n",
    "\n",
    "    for file in data_path:\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        file = os.path.join(f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{experiment}/player_level/', file)\n",
    "        print(file)\n",
    "        data = load_player_data(file)\n",
    "        root_visit_count = calculate_average_root_visit_count(data)\n",
    "        root_visit_counts.append(root_visit_count)\n",
    "\n",
    "        avg_max_depth = calculate_average_max_depth_reached(data)\n",
    "        avg_max_depths.append(avg_max_depth)\n",
    "\n",
    "        med_max_depth = calculate_median_max_depth_reached(data)\n",
    "        med_max_depths.append(med_max_depth)\n",
    "\n",
    "        max_max_depth = calculate_max_max_depth_reached(data)\n",
    "        max_max_depths.append(max_max_depth)\n",
    "\n",
    "    root_visit_counts_per_experiment[experiment] = np.mean(root_visit_counts)\n",
    "    avg_max_depth_reached_per_experiment[experiment] = np.mean(avg_max_depths)\n",
    "    med_max_depth_reached_per_experiment[experiment] = np.median(med_max_depths)\n",
    "    max_max_depth_reached_per_experiment[experiment] = np.max(max_max_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aee184",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_visit_counts_per_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ce713",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_max_depth_reached_per_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_max_depth_reached_per_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b883d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_max_depth_reached_per_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d4976",
   "metadata": {},
   "source": [
    "### Win rates for tuning experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e884d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_experiment = 'Paranoid_fortify_all_tuning'\n",
    "tuning_experiments_folder = f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{tuning_experiment}/'\n",
    "\n",
    "tuning_experiments = [f for f in os.listdir(tuning_experiments_folder) if os.path.isdir(os.path.join(tuning_experiments_folder, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3f919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_experiments.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc85d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paranoid_fortify_all_tuning_basic_evaluation_False',\n",
       " 'Paranoid_fortify_all_tuning_basic_evaluation_True',\n",
       " 'Paranoid_fortify_all_tuning_basic_heuristic_False',\n",
       " 'Paranoid_fortify_all_tuning_basic_heuristic_True',\n",
       " 'Paranoid_fortify_all_tuning_random_False',\n",
       " 'Paranoid_fortify_all_tuning_random_True']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4323555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates_all_experiments = {}\n",
    "\n",
    "for experiment in tuning_experiments:\n",
    "    data_path = f'/Users/moshalangerak/Documents/GitLab/risk-agent/data/experiments_results/{tuning_experiment}/{experiment}/game_level.csv'\n",
    "    data = load_data(data_path)\n",
    "    win_rates_per_player_type = calculate_win_rate_per_player_type(data)\n",
    "    win_rates_all_experiments[experiment] = win_rates_per_player_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c64d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates_all_experiments_df = pd.DataFrame(win_rates_all_experiments).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a631d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basic_evaluation</th>\n",
       "      <th>mcts</th>\n",
       "      <th>basic_heuristic</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Paranoid_fortify_all_tuning_basic_evaluation_False</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paranoid_fortify_all_tuning_basic_evaluation_True</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paranoid_fortify_all_tuning_basic_heuristic_False</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paranoid_fortify_all_tuning_basic_heuristic_True</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paranoid_fortify_all_tuning_random_False</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paranoid_fortify_all_tuning_random_True</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    basic_evaluation  mcts  \\\n",
       "Paranoid_fortify_all_tuning_basic_evaluation_False              0.76  0.22   \n",
       "Paranoid_fortify_all_tuning_basic_evaluation_True               0.81  0.16   \n",
       "Paranoid_fortify_all_tuning_basic_heuristic_False                NaN  0.62   \n",
       "Paranoid_fortify_all_tuning_basic_heuristic_True                 NaN  0.60   \n",
       "Paranoid_fortify_all_tuning_random_False                         NaN  0.72   \n",
       "Paranoid_fortify_all_tuning_random_True                          NaN  0.80   \n",
       "\n",
       "                                                    basic_heuristic  random  \n",
       "Paranoid_fortify_all_tuning_basic_evaluation_False              NaN     NaN  \n",
       "Paranoid_fortify_all_tuning_basic_evaluation_True               NaN     NaN  \n",
       "Paranoid_fortify_all_tuning_basic_heuristic_False              0.38     NaN  \n",
       "Paranoid_fortify_all_tuning_basic_heuristic_True               0.40     NaN  \n",
       "Paranoid_fortify_all_tuning_random_False                        NaN    0.28  \n",
       "Paranoid_fortify_all_tuning_random_True                         NaN    0.20  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_rates_all_experiments_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
